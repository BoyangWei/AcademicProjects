{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Description\tVariable Name\n",
    "#Spark Context\tsc\n",
    "#SQL Context / Hive Context\tsqlContext\n",
    "#SparkSession (2.0 Only)\tspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%fs ls /FileStore/tables/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%fs head /FileStore/tables/matches_time_result.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "inputPath = \"/FileStore/tables/matches.csv\"\n",
    "\n",
    "Schema = StructType([StructField(\"match_id\", StringType()), StructField(\"start_time\", TimestampType()), StructField(\"radiant_win\", StringType())])\n",
    "\n",
    "#staticInputDF = (  \n",
    "#  spark.read\n",
    "#    .option(\"header\", \"true\")\n",
    "#    .option(\"inferSchema\", \"true\")\n",
    "#    .option(\"delimiter\", \",\")\n",
    "#    .schema(Schema)\n",
    "#    .json(inputPath)\n",
    "#)\n",
    "df = spark.read.csv(inputPath, header=\"true\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df2 = df.select(\"start_time\", from_unixtime(df.start_time, 'MM/dd/yyyy').alias('date_time'))\n",
    "df = df.withColumn(\"date_time\",(from_unixtime(df.start_time, 'yyyy-MM-dd HH:mm:ss')))\n",
    "df.show()\n",
    "#to_date(df.start_time)\n",
    "\n",
    "dfCount = df.groupBy(\"radiant_win\").count()\n",
    "\n",
    "dfCount.createOrReplaceTempView(\"static_counts\")\n",
    "dfCount.cache()\n",
    "df.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "staticCountsDF = (\n",
    "  df.groupBy(\n",
    "       df.radiant_win, \n",
    "       window(df.date_time, \"4 hour\")\n",
    "  )\n",
    "    .count()\n",
    ")\n",
    "staticCountsDF.cache()\n",
    "\n",
    "# Register the DataFrame as table 'static_counts'\n",
    "staticCountsDF.createOrReplaceTempView(\"static_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(sql(\"select radiant_win, sum(count) as total_count from static_counts group by radiant_win\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql select radiant_win, date_format(window.end, \"MMM-dd HH:mm\") as time, count from static_counts order by time, radiant_win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputPath = \"/FileStore/tables/\"\n",
    "Schema = StructType([StructField(\"match_id\", StringType()), StructField(\"start_time\", StringType()), StructField(\"radiant_win\", StringType())])\n",
    "streamingInputDF = spark.readStream.csv(inputPath, schema = Schema)\n",
    "streamingInputDF = streamingInputDF.withColumn(\"date_time\",(from_unixtime(streamingInputDF.start_time, 'yyyy-MM-dd HH:mm:ss')))\n",
    "# Same query as staticInputDF\n",
    "#display(streamingInputDF)\n",
    "streamingCountsDF = (                 \n",
    "  streamingInputDF\n",
    "    .groupBy(\n",
    "      streamingInputDF.radiant_win, \n",
    "      window(streamingInputDF.date_time, \"1 hour\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# Is this DF actually a streaming DF?\n",
    "streamingCountsDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "  streamingCountsDF\n",
    "    .writeStream\n",
    "    .format(\"memory\")        \n",
    "    .queryName(\"counts\")     \n",
    "    .outputMode(\"complete\")  \n",
    "    .start()\n",
    ")\n",
    "display(streamingCountsDF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "name": "INFO7390_Assignment_4_Part_A&B",
  "notebookId": 3010453180968131
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
