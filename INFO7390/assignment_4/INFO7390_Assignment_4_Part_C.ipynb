{"cells":[{"cell_type":"code","source":["%fs ls /FileStore/tables/"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import *\n\n#inputPath = \"/FileStore/tables/match_info_5000_only_es.csv\"\ninputPath = \"/FileStore/tables/match_info_5000_es.csv\"\n\nSchema = StructType([StructField(\"match_id\", IntegerType()), \n                     StructField(\"hero_id\", IntegerType()), \n                     StructField(\"gold_per_min\", IntegerType()),\n                     StructField(\"xp_per_min\", IntegerType()),\n                     StructField(\"hero_damage\", IntegerType()),\n                     StructField(\"kills\", IntegerType()),\n                     StructField(\"assists\", IntegerType()),\n                     StructField(\"radiant_win\", StringType()),])\n\n#staticInputDF = (  \n#  spark.read\n#    .option(\"header\", \"true\")\n#    .option(\"inferSchema\", \"true\")\n#    .option(\"delimiter\", \",\")\n#    .schema(Schema)\n#    .json(inputPath)\n#)\ndf = spark.read.csv(inputPath, header=\"true\", schema=Schema)\ndf.printSchema()\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["display(df.select(\"*\"))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["categoricalColumns = [\"hero_id\"]\nstages = []\n\nfor categoricalCol in categoricalColumns:\n  # Category Indexing with StringIndexer\n  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n  # Add stages.  These are not run here, but will run all at once later on.\n  stages += [stringIndexer, encoder]"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["label_stringIdx = StringIndexer(inputCol = \"radiant_win\", outputCol = \"label\")\nstages += [label_stringIdx]"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Transform all features into a vector using VectorAssembler\nnumericCols = [\"gold_per_min\",\"xp_per_min\",\"hero_damage\",\"kills\",\"assists\"]\nassemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["pipeline = Pipeline(stages=stages)\npipelineModel = pipeline.fit(df)\ndataset = pipelineModel.transform(df)\n\ndataset.show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 999)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["predictions = lrModel.transform(testData)\npredictions.printSchema()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["selected = predictions.select(\"label\", \"prediction\", \"probability\", \"hero_id\", \"gold_per_min\", \"xp_per_min\", \"hero_damage\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["print lr.explainParams()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Create ParamGrid for Cross Validation\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lr.maxIter, [1, 5, 10])\n             .build())"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Create 10-fold CrossValidator\ncv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=10)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)\n# this will likely take a fair amount of time because of the amount of models that we're creating and testing"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["predictions = cvModel.transform(testData)\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\n\n# Create an initial RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n\n# Train model with Training Data\nrfModel = rf.fit(trainingData)\npredictions = rfModel.transform(testData)\npredictions.printSchema()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["selected = predictions.select(\"label\", \"prediction\", \"probability\", \"hero_id\", \"gold_per_min\", \"xp_per_min\", \"hero_damage\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(rf.maxDepth, [2, 4, 6])\n             .addGrid(rf.maxBins, [20, 60])\n             .addGrid(rf.numTrees, [5, 20])\n             .build())"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\ncvModel = cv.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["predictions = cvModel.transform(testData)\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["selected = predictions.select(\"label\", \"prediction\", \"probability\", \"hero_id\", \"gold_per_min\", \"xp_per_min\", \"hero_damage\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":24}],"metadata":{"name":"INFO7390_Assignment_4_Part_C","notebookId":135411681598110},"nbformat":4,"nbformat_minor":0}
