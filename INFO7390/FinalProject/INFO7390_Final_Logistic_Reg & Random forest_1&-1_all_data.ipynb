{"cells":[{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\n\nAP_data = pd.DataFrame(columns=[\"match_id\",\"hero_id\",\"player_slot\",\"radiant_win_res\"])\n#113 heros\nMatch_data_columns = range(1,114)\nMatch_data = pd.DataFrame(columns=Match_data_columns)\n\nprint(Match_data)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%fs ls /FileStore/tables/"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["df_data = pd.read_csv(\"/dbfs/FileStore/tables/match_hero_pick_all.csv\", header=0)\nprint(df_data)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["count = 1\nMatch_Result = pd.DataFrame(columns=[\"radiant_win_res\"])\nfor index, row in df_data.iterrows():\n  match_id = row['match_id']\n  hero_id = row['hero_id']\n  player_slot = row['player_slot']\n  radiant_win = row['radiant_win']\n  if radiant_win == True:\n      radiant_win_res = True\n  else:\n      radiant_win_res = False\n  Match_Result.loc[match_id] = radiant_win_res\n  if count % 10 == 1:\n      Match_data.loc[match_id] = np.zeros(113)\n  if count % 10 < 6:\n      Match_data.loc[match_id][hero_id] = 1\n  else:\n      Match_data.loc[match_id][hero_id] = -1\n  #Match_data.loc[match_id]['label'] = radiant_win_res\n  count += 1"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["Match_data['radiant_win'] = Match_Result.astype(int)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["Match_data.describe()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n\ncategoricalColumns = []\n#categoricalColumns = map(str, range(1,227))\n#categoricalColumns.remove('24')\n#categoricalColumns.remove('108')\n#categoricalColumns.remove('113')\n#categoricalColumns.remove('137')\n#categoricalColumns.remove('221')\n#categoricalColumns.remove('226')\n#print(categoricalColumns)\n\nstages = []\nfor categoricalCol in categoricalColumns:\n  # Category Indexing with StringIndexer\n  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n  # Add stages.  These are not run here, but will run all at once later on.\n  stages += [stringIndexer, encoder]"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["label_stringIdx = StringIndexer(inputCol = \"radiant_win\", outputCol = \"label\")\nstages += [label_stringIdx]"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["numericCols = map(str, range(1,114))\nassemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["spark_df = sqlContext.createDataFrame(Match_data)\n\npipeline = Pipeline(stages=stages)\npipelineModel = pipeline.fit(spark_df)\ndataset = pipelineModel.transform(spark_df)\n\ndisplay(dataset)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["(trainingData, testData) = dataset.randomSplit([0.91, 0.09], seed = 800)\ntrainingData.cache()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["display(lrModel, trainingData, \"ROC\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["display(lrModel, testData, \"ROC\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["predictions = lrModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["display(predictions)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Create ParamGrid for Cross Validation\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.05, 0.5, 1, 2])\n             .addGrid(lr.elasticNetParam, [0, 0.5, 1])\n             .addGrid(lr.maxIter, [10])\n             .build())"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# Create 10-fold CrossValidator\ncv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=10)\ncvModel = cv.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["predictions = cvModel.transform(testData)\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\n\n# Create an initial RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n\n# Train model with Training Data\nrfModel = rf.fit(trainingData)\npredictions = rfModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["display(predictions)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["print rf.explainParams()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(rf.maxDepth, [4, 6, 8, 10])\n             .addGrid(rf.maxBins, [20, 40, 60])\n             .addGrid(rf.numTrees, [20, 40])\n             .build())"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\ncvModel = cv.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["predictions = cvModel.transform(testData)\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":27}],"metadata":{"name":"INFO7390_Final_Logistic_Reg & Random forest_1&-1_all_data","notebookId":3318179771636308},"nbformat":4,"nbformat_minor":0}
